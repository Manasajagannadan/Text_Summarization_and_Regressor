# -*- coding: utf-8 -*-
"""rfp_for_general_insurance with SpaCy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdYsI5Qi62CeCTAKKM3Cc6Jyar5lU850
"""

!pip install PyPDF2
!pip install docx2txt

from google.colab import drive
drive.mount('/content/drive')

"""# Install & Import the necessary packages"""

import numpy as np
import PyPDF2
import docx2txt
import sys

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import networkx as nx
from nltk.tokenize.punkt import PunktSentenceTokenizer
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer

"""# Checking the format of the while whether it is in .txt or .pdf"""

def readDoc():
    name = '/content/drive/My Drive/work/rfp_for_general_insurance.pdf'
    print('You have asked for the document {}'.format(name))
    if name.lower().endswith('.txt'):
        choice = 1
    elif name.lower().endswith('.pdf'):
        choice = 2
    else:
        choice = 3
    print(choice)   
    if choice == 1:
        f = open(name, 'r')
        document = f.read()
        f.close()
    elif choice == 2:
        pdfFileObj = open(name, 'rb')
        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
        pageObj = pdfReader.getPage(0)
        document = pageObj.extractText()
        pdfFileObj.close()
    else:
        print('Failed to load a valid file')
        print('Returning an empty string')
        document = ''
    
    print(type(document))
    return document

document1 = readDoc()
print('The length of the file is:', end=' ')
print(len(document))

def tokenize(document):
    doc_tokenizer = PunktSentenceTokenizer()
    sentences_list = doc_tokenizer.tokenize(document)
    return sentences_list

sentences_list = tokenize(document)
print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))
print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))

print(type(sentences_list))

print('The size of the list "sentences" is: {}'.format(len(sentences_list)))

for i in sentences_list:
    print(i)

import spacy

from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

stopwords = list(STOP_WORDS)

nlp = spacy.load('en')

docx = nlp(document)

mytokens = [token.text for token in docx]

# word.text is tokenization in spacy
word_frequencies = {}
for word in docx:
    if word.text not in stopwords:
            if word.text not in word_frequencies.keys():
                word_frequencies[word.text] = 1
            else:
                word_frequencies[word.text] += 1

word_frequencies

maximum_frequency = max(word_frequencies.values())

for word in word_frequencies.keys():  
        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

word_frequencies

sentence_list = [ sentence for sentence in docx.sents ]

[w.text.lower() for t in sentence_list for w in t ]

sentence_scores = {}  
for sent in sentence_list:  
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if len(sent.text.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word.text.lower()]
                    else:
                        sentence_scores[sent] += word_frequencies[word.text.lower()]

sentence_scores

